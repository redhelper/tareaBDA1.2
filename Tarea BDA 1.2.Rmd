---
title: "Tarea 1.2 - Bases de Datos Avanzadas"
output: html_notebook
---

## Bases de datos Documentales
Rafael Calvo | Victor Franchis | Javier Levio
--- | --- | ---
201273506-8 | 201273566-1 | 201273598-k

### Preguntas:

- Cuales son los 5 países con los mejores vinos rankeados promedio?
- Cuales son los terminos descriptivos a los que mas se hacen alusion?
- Posibles relaciones entre ellos.
- Qué terminos son los mas comunes en 3 cepas de vino distintas?
- Generar una nube de palabras con los N terminos mas comunes.
- Es posible identificar clusters de algun tipo entre los datos analizados?

### Desarrollo:
*Importante! hubo consultas (pregunta 1) que solo funcionaron en mongo 3.4, favor de revisar su versión ya que la primera pregunta trajo problemasdebido a este factor.* 

Inicialmente se importa la libreria RMongo y nos conectamos a la BD "*BDA*". Recordar que esta fue creada en la parte 1.1 de la tarea con la coleccion "*wines_col*".
```{r}
library(RMongo)
mongo = mongoDbConnect("BDA", "localhost", 27017)
mongo
```
Para probar si funciona se realizara una consulta a los vinos de cepa *Tinta del Pais*.
```{r}
example = dbGetQuery(mongo, "wines_col", '{variety: "Gelber Traminer"}')
print(example, quote = TRUE, row.names = FALSE)
```
  
#### - Cuales son los 5 paises con los mejores vinos rankeados promedio?

```{r}
# library(RMongo)
# mongo = mongoDbConnect("BDA","localhost",27017)
output = dbAggregate(mongo,"wines_col", c(
  '{$group: {_id: "$country", promedio: {$avg: "$points"}}}',
  '{$sort: {promedio: -1}}',
  '{$limit: 5}'
))
print(output)
```
#### - Cuales son los terminos descriptivos a los que mas se hacen alusion?
#### - Posibles relaciones entre ellos.

#### - Que terminos son los mas comunes en 3 cepas de vino distintas?
*Nota: Se interpreta esta pregunta como el tomar 3 cepas a gusto y buscar los terminos mas comunes entre estas. De tener que analizarse estas por separado, la diferencia yace en tener que hacer 3 veces el mismo procedimiento, una vez por cada cepa a analizar.*

Primero se realiza una query para obtener solo los datos de 3 tipos de cepas:
```{r}
library(tm)
collection = dbGetQuery(mongo, "wines_col", '{$or: [{variety: "Tinta del Pais"},{variety: "Gelber Traminer"},{variety: "Terrantez"}]}')
descriptions = collection["description"][[1]]
```

Posteriormente lo transformamos a un *corpus* tal como se vio en clases. Como en este caso no se estan leyendo archivos se hace mediante la funcion *VectorSource*:
```{r}
docs = Corpus(VectorSource(descriptions))
```
Seguido de esto se crea un transformador de contenido llamado *toSpace* para eliminar todos los caracteres innecesarios que no correspondan a palabras, además de pasar cada letra a minúsculas y eliminar las *stopwords*:
```{r}
toSpace = content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))}) 
# eliminacion de caracteres
docs = tm_map(docs, toSpace, "-") 
docs = tm_map(docs, toSpace, ":") 
docs = tm_map(docs, removePunctuation) 
docs = tm_map(docs, toSpace, "'") 
docs = tm_map(docs, toSpace, "'") 
docs = tm_map(docs, toSpace, " - ") 
docs = tm_map(docs, toSpace, "-") 
docs = tm_map(docs, toSpace, "\n") 
docs = tm_map(docs, toSpace, "   ") 
docs = tm_map(docs, toSpace, "  ") 
# Transformar a minusculas 
docs = tm_map(docs,content_transformer(tolower)) 
# Eliminar digitos 
docs = tm_map(docs, removeNumbers) 
# Remover stopwords
docs = tm_map(docs, removeWords, stopwords("english")) 
# Espacios en blanco
docs = tm_map(docs, stripWhitespace) 
dtm= DocumentTermMatrix(docs)
dtm 
```

Seguido de esto se procede a Contar la frecuencia de ocurrencia de cada palabra en el corpus, sumando las filas:

```{r}
freq = colSums(as.matrix(dtm)) 
# Ordenar las frecuencias
ord = order(freq,decreasing=TRUE) 
# Listar los terminos mas frecuentes 
freq[head(ord)]
```

De esto se desprende que las palabras mas repetidas son palate y blackberry.


#### - Generar una nube de palabras con los N terminos mas comunes.
Se realiza una consulta similar a la inicial para obtener todas las descripciones:
```{r}
library(tm)
collection = dbGetQuery(mongo, "wines_col", '')
descriptions = collection["description"][[1]]
docs = Corpus(VectorSource(descriptions))
docs
```

Se puede revisar cada descripcion con el siguiente comando:
```{r}
writeLines(as.character(docs[[2]]))
```

Se procede a limpiar de caracteres y palabras innecesarias igual que en preguntas anteriores:
```{r}
docs = tm_map(docs, toSpace, "-") 
docs = tm_map(docs, toSpace, ":") 
docs = tm_map(docs, removePunctuation) 
docs = tm_map(docs, toSpace, "'") 
docs = tm_map(docs, toSpace, "'") 
docs = tm_map(docs, toSpace, " - ") 
docs = tm_map(docs, toSpace, "-") 
docs = tm_map(docs, toSpace, "\n") 
docs = tm_map(docs, toSpace, "   ") 
docs = tm_map(docs, toSpace, "  ")
docs = tm_map(docs,content_transformer(tolower))
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, stripWhitespace) 
```

Nuevamente revisamos la descripcion para ver si hubo cambios:
```{r}
# Inspeccionar el nuevo contenido
writeLines(as.character(docs[[2]]))
```

Por ahora no se considerara el uso de stemming, por lo que se pasara  directo a la construccion de una matriz de documentos y terminos:
```{r}
dtm= DocumentTermMatrix(docs)
dtm 
```
Revisando el contenido se observa que hubo resultados:
```{r}

inspect(dtm[10:15,1:7]) 
```
Se procede a realizar analisis de texto. Al sumar la frecuencia de ocurrencia de cada palabra en cada documento se obtiene una lista de frecuencia de palabras. De resultar todo correctamente se debería tener un largo de la lista igual al ancho de la matriz anterior:
```{r}
freq = colSums(as.matrix(dtm))
length(freq) 
```
Procedemos finalmente a graficar los datos, para ello necesitamos de una semilla, con lo cual ya se obtiene la nube de palabras:

```{r}
library(wordcloud) 
library(RColorBrewer)
# Setear un valor semilla 
set.seed(42) 
# Nube de palabras en colores; palabras con frecuencia m?nima de 70 
wordcloud(names(freq), freq,min.freq=70,colors=brewer.pal(6,"Dark2")) 
```


#### Es posible identificar clusters de algun tipo entre los datos analizados?
Es posible, pero al eliminar valores que no son muy usados la mayoría desaparece. Para respetar el proceso visto en clases se hará este proceso, pero con valores muy grandes de "sparse". se observa que comienzan a aparecer a los **0.6**, se usará *0.9* para tener suficientes datos.

```{r}
dtmr2 = removeSparseTerms(dtm, sparse = 0.9) 
inspect(dtmr2)
```


Se procede entonces a buscar clustering por particionamiento. Esto se hará usando Kmeans con un numero de clusters de 3:
```{r}
library(fpc) 
nroClusters = 3
kfit = kmeans(distMatrix, nroClusters) 
clusplot(as.matrix(distMatrix), kfit$cluster, color=T, shade=T, labels=2, lines=0, main="Clusters de terminos") 
```
Evidentemente la palabra mas usada es vino, lo cual era de esperarse dado el contexto.

Como adicional se revisan las palabras representativas dentro de cada uno de los clusters: 
```{r}
# Para chequear palabras representativas dentro de cada cluster
for (i in 1: nroClusters) 
{ 
  cat(paste("cluster ", i, ": ", sep = "")) 
  s = sort(kfit$centers[i,], decreasing = T) 
  cat(names(s)[1:5], "\n") 
} 
```


> Nota: Este documento se encuentra escrito sin tildes ni caracteres especiales a proposito, con el fin de evitar problemas de formato.









# todo esto es extra, lo dejo en caso de que sea util para las otras preguntas
### Graficos
```{r}
library(stringi) 
library(ggplot2) 
library(Rcpp)
```
```{r}
# Crear un marco de datos, con columnas de igual largo 
wf = data.frame(term=names(freq),occurrences=freq) 
# Dibujar t?rminos que aparecen m?s de 150 veces 
p = ggplot(subset(wf, freq>150), aes(term, occurrences)) 
# "Identity" asegura que la altura de cada barra es proporcional al valor del datos mapeado en el eje y
p = p + geom_bar(stat="identity") 
# Especificar que las etiquetas del eje x se muestren en un ?ngulo de 45 grados y horizon- 
# talmente justificada (chequear si no fuera as?, es decir: angle = 0). 
p = p + theme(axis.text.x=element_text(angle=45, hjust=1)) 
p 
```

```{r}
# De ser necesario, seleccionar repositorio: BioC software, para esta secci?n 
# https://www.bioconductor.org/packages/release/bioc/html/graph.html
#source("https://bioconductor.org/biocLite.R")
#biocLite("graph")
#source("https://bioconductor.org/biocLite.R")
#biocLite("Rgraphviz")

```
```{r}
#library(graph) 
#library(Rgraphviz) 
# Dibujar relaciones entre t?rminos que tengan una frecuencia no menor a 160 
#freq.terms = findFreqTerms(dtm,lowfreq=160) 
#plot(dtm, term = freq.terms, corThreshold = 0.12, weighting = T) 
```


